<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://conflictednerd.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://conflictednerd.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-05T19:59:51+00:00</updated><id>https://conflictednerd.github.io/feed.xml</id><title type="html">Saeed Hedayatian</title><subtitle>PhD student in Computer Science at University of Southern California </subtitle><entry><title type="html"></title><link href="https://conflictednerd.github.io/blog/2025/2023-02-19-In-Praise-of-einsum/" rel="alternate" type="text/html" title=""/><published>2025-06-05T19:59:51+00:00</published><updated>2025-06-05T19:59:51+00:00</updated><id>https://conflictednerd.github.io/blog/2025/2023-02-19-In-Praise-of-einsum</id><content type="html" xml:base="https://conflictednerd.github.io/blog/2025/2023-02-19-In-Praise-of-einsum/"><![CDATA[<p>This is a short note about the <code class="language-plaintext highlighter-rouge">einsum</code> functionality that is present in numpy, jax, etc. Understanding what it does is a bit tricky –naturally, because it can do the job of many other functions– but it is also very rewarding as it can help a lot with linear algebraic computations. I will use numpy’s <code class="language-plaintext highlighter-rouge">np.einsum()</code> notation, but the underlying concepts are the same regardless of slight syntactic differences in libraries.</p> <p>Put simply, <code class="language-plaintext highlighter-rouge">einsum</code> allows you to convert simple sums over product of elements of matrices from math into code. It can also change the order of axes of a matrix (computing transposes). For instance, let’s say we want to compute the product of two matrices $A, B$ and store them in a new matrix $C$. Mathematically, we can specify $C$ as \(C\_{ik} = \sum\_{j} A\_{ij}B\_{jk}.\) In code, this could be written as <code class="language-plaintext highlighter-rouge">C = np.einsum('ij, jk -&gt; ik', A, B)</code>. First, I will discuss exactly where we can use <code class="language-plaintext highlighter-rouge">einsum</code>. Then, I’ll explain how to use <code class="language-plaintext highlighter-rouge">einsum</code> to convert math to code and vice versa. Finally, I will show how to use <code class="language-plaintext highlighter-rouge">einsum</code> to perform some everyday linear algebraic computations.</p> <h2 id="where">Where?</h2> <p>Generally, speaking whenever we have a number of tensors $A^1, \cdots, A^n$ and we want to obtain a new tensor $B$ whose elements can be specified in the general form of \(B\_{i\_1, \cdots, i\_k} = \sum\_{j\_1, \cdots, j\_l} A^1A^2\cdots A^n\) then we can use <code class="language-plaintext highlighter-rouge">einsum</code> to code up the operations. This general form, which I like to call the <em>sum-of-product</em> form comes up frequently in linear algebraic computations. (Batch) matrix multiplication, matrix-vector product, vector outer-product, computing the trace, etc. are all computations that can be written in this form. In the examples section further down, a number of these applications are discussed.</p> <h2 id="how">How?</h2> <p>The <code class="language-plaintext highlighter-rouge">einsum</code> function takes two sets of arguments; a specification string and the tensors over which the computations are to be performed. The output of this function is a tensor containing the result. Remember the code for matrix multiplication: <code class="language-plaintext highlighter-rouge">C = np.einsum('ij, jk -&gt; ik', A, B)</code>. Here, <code class="language-plaintext highlighter-rouge">'ij, jk -&gt; ik'</code> is the specification string, <code class="language-plaintext highlighter-rouge">A, B</code> are input tensors and <code class="language-plaintext highlighter-rouge">C</code> is the output. In the explanations below, I will use this matrix multiplication code.</p> <p>Let’s see how we can interpret the specification string. We can easily convert any <code class="language-plaintext highlighter-rouge">einsum</code> code into a sum-of-product formula by doing the following:</p> <ol> <li>Write down the expression for multiplying the elements of the <strong>input tensors</strong> specified by the indices that are on the <strong>left hand side</strong> of <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol in the specification string. For the matrix multiplication example, this would be \(A\_{ij}B\_{jk}.\)</li> <li>Write down the element of the output tensor specified by the indices on the <strong>right hand side</strong> of the <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol. For the matrix multiplication example, this would be \(C\_{ik}.\)</li> <li>Next we have to identify what I call <em>slack indices</em>. These are the indices that are used on the left hand side of the <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol, but not on the right hand side. In other words, these are the indices that were used for the inputs in step 1, but not for the output in step 2. For the matrix multiplication example this would be the <code class="language-plaintext highlighter-rouge">j</code> index.</li> <li>Compute the sum of the expression in step 1 over all the slack indices to get the element of the output in step 2. For the matrix multiplication example we would write \(\underbrace{C\_{ik}}\_{\text{step 2}} = \sum\_{\underbrace{j}\_{\text{slack index}}} \underbrace{A\_{ij}B\_{jk}}\_{\text{step 1}}.\) And we are done! Of course, we can convert any sum-of-product expression into an <code class="language-plaintext highlighter-rouge">einops</code> code following the same logic. See the examples in the next section to get a better grip on these. There are a couple of things that you should keep in mind: <ul> <li>In the specification string, there shouldn’t be any index on the right hand side that is not used on the left hand side. So we can’t have something like <code class="language-plaintext highlighter-rouge">ij, jk -&gt; il</code> because <code class="language-plaintext highlighter-rouge">l</code> is not used on the left hand side. This is logical if you think about it in terms of the corresponding sum-of-product equation.</li> <li>Dimensions of the input that correspond to the same indices in the specification string should be the same. In the matrix multiplication example, the index <code class="language-plaintext highlighter-rouge">j</code> is used in for both inputs <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>. Because of this, the corresponding dimensions (first dimension of <code class="language-plaintext highlighter-rouge">A</code> and second dimension of <code class="language-plaintext highlighter-rouge">B</code>) must have the same length, which is as if we were saying two matrices $A_{mn}, B_{kl}$ can be multiplied together, only if $n=k$.</li> </ul> </li> </ol> <h2 id="examples">Examples</h2> <h3 id="matrix-vector-product">Matrix Vector Product</h3> <p>If we have a vector $v = [v_1, \cdots, v_n]^T$ and an $m\times n$ matrix $A$, then $u = Av$ is an $m\times 1$ vector whose $i$-th element is the dot product of $v$ with the $i$-th row of $A$. Mathematically, we can represent $u$ in a sum-of-product form: \(u\_i = \sum\_j A\_{ij}v\_j.\) So the specification string is <code class="language-plaintext highlighter-rouge">'ij, j -&gt; i'</code>. Notice that here $j$ is a slack index that we sum over. The final code is <code class="language-plaintext highlighter-rouge">u = np.einsum('ij, j -&gt; i', A, v)</code>.</p> <h3 id="inner-product">Inner Product</h3> <p>The inner product of two $n$ dimensional vectors $u$ and $v$ is a single scalar $p$ which has a sum-of-product form: \(p = \sum\_i u\_iv\_i.\) This means that the corresponding specification string is <code class="language-plaintext highlighter-rouge">i, i -&gt;</code>. Notice that there is no index on the right hand side of this string, which means that the output is a scalar and that $i$ is an slack index that we sum over. The final code is <code class="language-plaintext highlighter-rouge">p = np.einsum('i, i -&gt; ', u, v)</code>.</p> <h3 id="outer-product">Outer Product</h3> <p>If we have an $m$ dimensional vector $u$ and an $n$ dimensional vector $v$, then their outer product $A = u \otimes v$ is a rank-1, $m\times n$ matrix where the $i$-th column is $u$ multiplied by the $i$-th element of $v$. We can represent $A$ in the sum-of-product form: \(A\_{ij} = u\_i v\_j.\) In the code, we can compute it by <code class="language-plaintext highlighter-rouge">A = np.einsum('i, j -&gt; ij', u, v)</code>. Notice that here there are no slack indices.</p> <h3 id="row-sum-and-column-sum">Row Sum and Column Sum</h3> <p>We can use <code class="language-plaintext highlighter-rouge">einsum</code> to compute the sum of all elements in each row. For a matrix $A$, the result would be a vector $r$ where \(r\_i = \sum\_j A\_{ij}.\) We can turn this into a specification string and write <code class="language-plaintext highlighter-rouge">r = np.einsum('ij -&gt; i', A)</code>. Similarly, to compute the sum of all elements in each column we can use <code class="language-plaintext highlighter-rouge">c = np.einsum('ij -&gt; j', A)</code>. When we have multi-dimensional tensors and we want to compute their sum over an axis, <code class="language-plaintext highlighter-rouge">einsum</code> notation could help with the clarity of the code. For instance, if we write</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y = np.einsum('BCmn -&gt; BC', X)
</code></pre></div></div> <p>we can immediately say that <code class="language-plaintext highlighter-rouge">A</code> has the shape <code class="language-plaintext highlighter-rouge">B x C x m x n</code> (perhaps a batch of <code class="language-plaintext highlighter-rouge">B</code> images, each with <code class="language-plaintext highlighter-rouge">C</code> channels and size <code class="language-plaintext highlighter-rouge">m x n</code>) and for each channel in each batch, we have computed the sum of all elements to arrive at a tensor that has the shape <code class="language-plaintext highlighter-rouge">B x C</code>. Contrast this with</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y = np.sum(X, axis=(2,3))
</code></pre></div></div> <p>which does the same job. So <code class="language-plaintext highlighter-rouge">einsum</code> could help you track the shapes as well!</p> <h3 id="trace">Trace</h3> <p>The trace of a matrix $A$ is a scalar $t$ which is the sum of all elements on its main diagonal. In the sum-of-product form, this is represented by \(t = \sum\_i A\_{ii}.\) This can be coded as <code class="language-plaintext highlighter-rouge">t = np.einsum('ii -&gt; ', A)</code>. Notice here how the index $i$ is used twice for referencing the same input argument.</p> <h3 id="main-diagonal">Main Diagonal</h3> <p>Similar to the way we computed the trace, we can extract the main diagonal of a matrix as a vector. In the sum-of-product form, the main diagonal can be seen as \(d\_i = A\_{ii}.\) We can code this as <code class="language-plaintext highlighter-rouge">d = np.einsum('ii -&gt; i', A)</code>.</p> <h3 id="transpose">Transpose</h3> <p>Computing the transpose of a matrix $A$ is also very easy using <code class="language-plaintext highlighter-rouge">einsum</code>. The sum-of-product notation would simply be $B_{ji} = A_{ij}$ and the corresponding code is <code class="language-plaintext highlighter-rouge">B = np.einsum('ij -&gt; ji', A)</code>.</p> <h3 id="batch-matrix-multiplication">Batch Matrix Multiplication</h3> <p>Adding one (or more) batch dimension is very easy using the <code class="language-plaintext highlighter-rouge">einsum</code> notation. if <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> are batches of matrices (batch index comes first) that are to be multiplied, then we can write <code class="language-plaintext highlighter-rouge">C = np.einsum('nij, njk -&gt; nik', A, B)</code>. If we write down the corresponding sum-of-product expression, it becomes evident that the batch index just acts as a counter, not involved in the computations. \(C\_{nik} = \sum\_{j} A\_{nij}B\_{njk}\) So for the first elements in the batch $n=0$, we would have \(C\_{0,ik} = \sum\_{j} A\_{0,ij}B\_{0,jk}.\) Which means that the first element in the output batch, <code class="language-plaintext highlighter-rouge">C[0]</code>, is the matrix product of <code class="language-plaintext highlighter-rouge">A[0]</code> and <code class="language-plaintext highlighter-rouge">B[0]</code>. We can similarly add a batch dimension to any of the other examples. There is also one nice trick when we have more than one batch dimension. We can write <code class="language-plaintext highlighter-rouge">np.einsum('...ij, ...ji -&gt; ...ik', A, B)</code> to avoid explicitly writing all batch dimensions that proceed the last two dimensions, over which we want to perform the multiplication.</p>]]></content><author><name></name></author></entry></feed>