<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://conflictednerd.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://conflictednerd.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-05T20:27:24+00:00</updated><id>https://conflictednerd.github.io/feed.xml</id><title type="html">Saeed Hedayatian</title><subtitle>PhD student in Computer Science at University of Southern California </subtitle><entry><title type="html">In Praise of Einsum</title><link href="https://conflictednerd.github.io/blog/2023/In-Praise-of-einsum/" rel="alternate" type="text/html" title="In Praise of Einsum"/><published>2023-02-19T09:10:17+00:00</published><updated>2023-02-19T09:10:17+00:00</updated><id>https://conflictednerd.github.io/blog/2023/In-Praise-of-einsum</id><content type="html" xml:base="https://conflictednerd.github.io/blog/2023/In-Praise-of-einsum/"><![CDATA[<p>This is a short note about the <code class="language-plaintext highlighter-rouge">einsum</code> functionality that is present in numpy, jax, etc. Understanding what it does is a bit tricky –naturally, because it can do the job of many other functions– but it is also very rewarding as it can help a lot with linear algebraic computations. I will use numpy’s <code class="language-plaintext highlighter-rouge">np.einsum()</code> notation, but the underlying concepts are the same regardless of slight syntactic differences in libraries.</p> <p>Put simply, <code class="language-plaintext highlighter-rouge">einsum</code> allows you to convert simple sums over product of elements of matrices from math into code. It can also change the order of axes of a matrix (computing transposes). For instance, let’s say we want to compute the product of two matrices $A, B$ and store them in a new matrix $C$. Mathematically, we can specify $C$ as \(C_{ik} = \sum_{j} A_{ij}B_{jk}.\) In code, this could be written as <code class="language-plaintext highlighter-rouge">C = np.einsum('ij, jk -&gt; ik', A, B)</code>. First, I will discuss exactly where we can use <code class="language-plaintext highlighter-rouge">einsum</code>. Then, I’ll explain how to use <code class="language-plaintext highlighter-rouge">einsum</code> to convert math to code and vice versa. Finally, I will show how to use <code class="language-plaintext highlighter-rouge">einsum</code> to perform some everyday linear algebraic computations.</p> <h2 id="where">Where?</h2> <p>Generally, speaking whenever we have a number of tensors $A^1, \ldots, A^n$ and we want to obtain a new tensor $B$ whose elements can be specified in the general form of \(B_{i_1, \ldots, i_k} = \sum_{j_1, \ldots, j_l} A^1 A^2 \ldots A^n\) then we can use <code class="language-plaintext highlighter-rouge">einsum</code> to code up the operations. This general form, which I like to call the <em>sum-of-product</em> form comes up frequently in linear algebraic computations. (Batch) matrix multiplication, matrix-vector product, vector outer-product, computing the trace, etc. are all computations that can be written in this form. In the examples section further down, a number of these applications are discussed.</p> <h2 id="how">How?</h2> <p>The <code class="language-plaintext highlighter-rouge">einsum</code> function takes two sets of arguments; a specification string and the tensors over which the computations are to be performed. The output of this function is a tensor containing the result. Remember the code for matrix multiplication: <code class="language-plaintext highlighter-rouge">C = np.einsum('ij, jk -&gt; ik', A, B)</code>. Here, <code class="language-plaintext highlighter-rouge">'ij, jk -&gt; ik'</code> is the specification string, <code class="language-plaintext highlighter-rouge">A, B</code> are input tensors and <code class="language-plaintext highlighter-rouge">C</code> is the output. In the explanations below, I will use this matrix multiplication code.</p> <p>Let’s see how we can interpret the specification string. We can easily convert any <code class="language-plaintext highlighter-rouge">einsum</code> code into a sum-of-product formula by doing the following:</p> <ol> <li>Write down the expression for multiplying the elements of the <strong>input tensors</strong> specified by the indices that are on the <strong>left hand side</strong> of <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol in the specification string. For the matrix multiplication example, this would be \(A_{ij} B_{jk}.\)</li> <li>Write down the element of the output tensor specified by the indices on the <strong>right hand side</strong> of the <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol. For the matrix multiplication example, this would be \(C_{ik}.\)</li> <li>Next we have to identify what I call <em>slack indices</em>. These are the indices that are used on the left hand side of the <code class="language-plaintext highlighter-rouge">-&gt;</code> symbol, but not on the right hand side. In other words, these are the indices that were used for the inputs in step 1, but not for the output in step 2. For the matrix multiplication example this would be the <code class="language-plaintext highlighter-rouge">j</code> index.</li> <li>Compute the sum of the expression in step 1 over all the slack indices to get the element of the output in step 2. For the matrix multiplication example we would write \(\underbrace{C_{ik}}_{\text{step 2}} = \sum_{\underbrace{j}_{\text{slack index}}} \underbrace{A_{ij}B_{jk}}_{\text{step 1}}.\) And we are done! Of course, we can convert any sum-of-product expression into an <code class="language-plaintext highlighter-rouge">einops</code> code following the same logic. See the examples in the next section to get a better grip on these. There are a couple of things that you should keep in mind: <ul> <li>In the specification string, there shouldn’t be any index on the right hand side that is not used on the left hand side. So we can’t have something like <code class="language-plaintext highlighter-rouge">ij, jk -&gt; il</code> because <code class="language-plaintext highlighter-rouge">l</code> is not used on the left hand side. This is logical if you think about it in terms of the corresponding sum-of-product equation.</li> <li>Dimensions of the input that correspond to the same indices in the specification string should be the same. In the matrix multiplication example, the index <code class="language-plaintext highlighter-rouge">j</code> is used in for both inputs <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>. Because of this, the corresponding dimensions (first dimension of <code class="language-plaintext highlighter-rouge">A</code> and second dimension of <code class="language-plaintext highlighter-rouge">B</code>) must have the same length, which is as if we were saying two matrices $A_{mn}, B_{kl}$ can be multiplied together, only if $n=k$.</li> </ul> </li> </ol> <h2 id="examples">Examples</h2> <h3 id="matrix-vector-product">Matrix Vector Product</h3> <p>If we have a vector $v = [v_1, \cdots, v_n]^T$ and an $m \times n$ matrix $A$, then $u = Av$ is an $m \times 1$ vector whose $i$-th element is the dot product of $v$ with the $i$-th row of $A$. Mathematically, we can represent $u$ in a sum-of-product form: \(u_i = \sum_j A_{ij} v_j.\) So the specification string is <code class="language-plaintext highlighter-rouge">'ij, j -&gt; i'</code>. Notice that here $j$ is a slack index that we sum over. The final code is <code class="language-plaintext highlighter-rouge">u = np.einsum('ij, j -&gt; i', A, v)</code>.</p> <h3 id="inner-product">Inner Product</h3> <p>The inner product of two $n$ dimensional vectors $u$ and $v$ is a single scalar $p$ which has a sum-of-product form: \(p = \sum_i u_i v_i.\) This means that the corresponding specification string is <code class="language-plaintext highlighter-rouge">i, i -&gt;</code>. Notice that there is no index on the right hand side of this string, which means that the output is a scalar and that $i$ is an slack index that we sum over. The final code is <code class="language-plaintext highlighter-rouge">p = np.einsum('i, i -&gt; ', u, v)</code>.</p> <h3 id="outer-product">Outer Product</h3> <p>If we have an $m$ dimensional vector $u$ and an $n$ dimensional vector $v$, then their outer product $A = u \otimes v$ is a rank-1, $m\times n$ matrix where the $i$-th column is $u$ multiplied by the $i$-th element of $v$. We can represent $A$ in the sum-of-product form: \(A_{ij} = u_i v_j.\) In the code, we can compute it by <code class="language-plaintext highlighter-rouge">A = np.einsum('i, j -&gt; ij', u, v)</code>. Notice that here there are no slack indices.</p> <h3 id="row-sum-and-column-sum">Row Sum and Column Sum</h3> <p>We can use <code class="language-plaintext highlighter-rouge">einsum</code> to compute the sum of all elements in each row. For a matrix $A$, the result would be a vector $r$ where \(r_i = \sum_j A_{ij}.\) We can turn this into a specification string and write <code class="language-plaintext highlighter-rouge">r = np.einsum('ij -&gt; i', A)</code>. Similarly, to compute the sum of all elements in each column we can use <code class="language-plaintext highlighter-rouge">c = np.einsum('ij -&gt; j', A)</code>. When we have multi-dimensional tensors and we want to compute their sum over an axis, <code class="language-plaintext highlighter-rouge">einsum</code> notation could help with the clarity of the code. For instance, if we write</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y = np.einsum('BCmn -&gt; BC', X)
</code></pre></div></div> <p>we can immediately say that <code class="language-plaintext highlighter-rouge">A</code> has the shape <code class="language-plaintext highlighter-rouge">B x C x m x n</code> (perhaps a batch of <code class="language-plaintext highlighter-rouge">B</code> images, each with <code class="language-plaintext highlighter-rouge">C</code> channels and size <code class="language-plaintext highlighter-rouge">m x n</code>) and for each channel in each batch, we have computed the sum of all elements to arrive at a tensor that has the shape <code class="language-plaintext highlighter-rouge">B x C</code>. Contrast this with</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y = np.sum(X, axis=(2,3))
</code></pre></div></div> <p>which does the same job. So <code class="language-plaintext highlighter-rouge">einsum</code> could help you track the shapes as well!</p> <h3 id="trace">Trace</h3> <p>The trace of a matrix $A$ is a scalar $t$ which is the sum of all elements on its main diagonal. In the sum-of-product form, this is represented by \(t = \sum_i A_{ii}.\) This can be coded as <code class="language-plaintext highlighter-rouge">t = np.einsum('ii -&gt; ', A)</code>. Notice here how the index $i$ is used twice for referencing the same input argument.</p> <h3 id="main-diagonal">Main Diagonal</h3> <p>Similar to the way we computed the trace, we can extract the main diagonal of a matrix as a vector. In the sum-of-product form, the main diagonal can be seen as \(d_i = A_{ii}.\) We can code this as <code class="language-plaintext highlighter-rouge">d = np.einsum('ii -&gt; i', A)</code>.</p> <h3 id="transpose">Transpose</h3> <p>Computing the transpose of a matrix $A$ is also very easy using <code class="language-plaintext highlighter-rouge">einsum</code>. The sum-of-product notation would simply be $B_{ji} = A_{ij}$ and the corresponding code is <code class="language-plaintext highlighter-rouge">B = np.einsum('ij -&gt; ji', A)</code>.</p> <h3 id="batch-matrix-multiplication">Batch Matrix Multiplication</h3> <p>Adding one (or more) batch dimension is very easy using the <code class="language-plaintext highlighter-rouge">einsum</code> notation. if <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> are batches of matrices (batch index comes first) that are to be multiplied, then we can write <code class="language-plaintext highlighter-rouge">C = np.einsum('nij, njk -&gt; nik', A, B)</code>. If we write down the corresponding sum-of-product expression, it becomes evident that the batch index just acts as a counter, not involved in the computations. \(C_{nik} = \sum_{j} A_{nij}B_{njk}\) So for the first elements in the batch $n=0$, we would have \(C_{0,ik} = \sum_{j} A_{0,ij}B_{0,jk}.\) Which means that the first element in the output batch, <code class="language-plaintext highlighter-rouge">C[0]</code>, is the matrix product of <code class="language-plaintext highlighter-rouge">A[0]</code> and <code class="language-plaintext highlighter-rouge">B[0]</code>. We can similarly add a batch dimension to any of the other examples. There is also one nice trick when we have more than one batch dimension. We can write <code class="language-plaintext highlighter-rouge">np.einsum('...ij, ...ji -&gt; ...ik', A, B)</code> to avoid explicitly writing all batch dimensions that proceed the last two dimensions, over which we want to perform the multiplication.</p>]]></content><author><name></name></author><category term="blog"/><category term="jax"/><category term="numpy"/><category term="machine-learning"/><summary type="html"><![CDATA[A tutorial on einsum, Einstein summation notation.]]></summary></entry><entry><title type="html">Introduction to Information Theory</title><link href="https://conflictednerd.github.io/blog/2022/Introduction-to-Information-Theory/" rel="alternate" type="text/html" title="Introduction to Information Theory"/><published>2022-07-25T20:22:16+00:00</published><updated>2022-07-25T20:22:16+00:00</updated><id>https://conflictednerd.github.io/blog/2022/Introduction-to-Information-Theory</id><content type="html" xml:base="https://conflictednerd.github.io/blog/2022/Introduction-to-Information-Theory/"><![CDATA[<p>Some of the notes I wrote when I took an information theory course. It contains basic definitions and theorems. It doesn’t get into much details and is sort of an intuitive cheat-sheet. The content aligns with the first few chapters of the book <em>“Elements of Information Theory”</em> by Cover and Thomas, so you can find the proofs there (it is a very well-written and easy-to-read book, I definitely recommend it!).</p> <h2 id="entropy">Entropy</h2> <p>Let $X$ be a discrete random variable with alphabet $\mathcal{X}$ and probability mass function $p(x)$.</p> <p>The <strong>entropy</strong> $H(X)$ of a discrete random variable $X$ is defined by \(H(X) = -\mathbb{E}_{X\sim p(x)}\left[\log p(X)\right] = - \sum\_{x\in\mathcal{X}}p(x)\log p(x).\)</p> <ul> <li> <p>Intuitively, the entropy $H(X)$ is a measure of how <em>informative</em> the observation of an instantiation of $X$ is. Alternatively, we can think of it as how <em>uncertain</em> we are about the value of $X$.</p> </li> <li> <p>If $X$ is not random at all, we already know what an instantiation is going to be; therefore, the entropy is zero. On the other hand, the more uniformly distributed $p(x)$ is, the more uncertain we are about the value of an instantiation of $X$.</p> </li> <li> <p>When calculating the entropy, we don’t consider the values of $x\in \mathcal{X}$ for which $p(x) = 0$. So, conventionally, $0\log 0 = 0\log\infty = 0$ and $p\log\infty = \infty$ in the calculations.</p> </li> </ul> <h2 id="joint-and-conditional-entropy">Joint and Conditional Entropy</h2> <p>The <strong>joint entropy</strong> $H(X, Y)$ of a pair of random variables is just the entropy of their joint distribution:</p> \[H(X, Y) = -\mathbb{E}_{X\sim p(x), Y\sim p(y)}\left[\log p(X, Y)\right] = \sum\_{x\in\mathcal{X}, y\in\mathcal{Y}}p(x, y)\log p(x, y).\] <p>The <strong>conditional entropy</strong> of $Y$ given $X$ is defined as the expected entropy of $Y$ given the value of $X$:</p> \[H(Y|X) = -\mathbb{E}_{X\sim p(x), Y\sim p(y)}\left[\log p(Y|X)\right] \\ = -\sum\_{x\in\mathcal{X}, y\in \mathcal{Y}}p(x, y)\log p(y|x) \\ = \sum\_{x\in \mathcal{X}}p(x)H(Y|X=x).\] <ul> <li> <table> <tbody> <tr> <td>In general, $H(Y</td> <td>X) \neq H(X</td> <td>Y)$. For example, consider $X \sim \mathrm{Uniform} ({1, \cdots, 100})$ and $Y=X \;\mathrm{mod}\; 10$.</td> </tr> </tbody> </table> </li> </ul> <p>Because of the natural definition of entropy, many of the theorems about probability distributions translate naturally for entropy. For instance, we have the <strong>chain rule</strong>:</p> \[H(X, Y) = H(X) + H(Y|X)\] <h2 id="kl-divergence-and-mutual-information">KL-divergence and Mutual Information</h2> <p>The <strong>Kullback-Leibler divergence</strong> or <strong>relative entropy</strong> between two probability mass functions $p(x)$ and $q(x)$ is defined as</p> \[D(p||q) = \mathbb{E}_{X\sim p(X)}\left[\log\frac{p(X)}{q(X)}\right] = \sum\_{x\in \mathcal{X}} p(x)\log\frac{p(x)}{q(x)}.\] <ul> <li> <p>In calculations, let $0\log {(\mathrm{whatever})} = 0$ and $p\log\frac{p}{0} = \infty$ for $p&gt;0$.</p> </li> <li> <p>You can think of KL-divergence as some sort of distance between $p$ and $q$. However, it is not a metric, as it is not symmetric and does not satisfy the triangle inequality.</p> </li> <li> <p>Another way of thinking about it is as <em>a measure of how similar samples of $p$ are to those of $q$</em>.</p> </li> <li> <table> <tbody> <tr> <td>If there is a symbol $x\in \mathcal{X}$ that may be seen when we sample from $p$, but will never appear when sampling from $q$ (i.e., $p(x)&gt;0 = q(x)$), then the distance $D(p</td> <td> </td> <td>q)$ is $\infty$. This is because there is a chance that, when sampling from $p$, we observe $x$, and immediately recognize that we are not sampling from $q$. Note however, that $D(q</td> <td> </td> <td>p)$ may not be $\infty$, as not observing $x$ can not assure us that we are sampling form $p$.</td> </tr> </tbody> </table> </li> </ul> <p>The <strong>mutual information</strong> $I(X; Y)$ between two random variables is the KL-divergence between their joint distribution, $p(x, y)$ and the product of their marginals, $p(x)p(y)$:</p> \[I(X; Y) = D(p(x, y)||p(x)p(y)) = \sum_{x\in\mathcal{X}, y\in \mathcal{Y}} p(x, y)\log\frac{p(x, y)}{p(x)p(y)}.\] <ul> <li> <p>Mutual information is symmetric, i.e., $I(X;Y) = I(Y;X).$</p> </li> <li> <p>You can think of mutual information as the <em>amount of shared information between two random variables</em>. If $X$ and $Y$ are independent, their mutual information is zero. Otherwise, their mutual information is <em>strictly positive</em>.</p> </li> <li> <p>If $Y = f(X) $ for any one-to-one mapping $f$, then $X$ and $Y$ contain the same information; hence, their mutual information is equal to (each of) their entropies.</p> </li> <li> <p>In particular, the amount of shared information between $X$ and itself, is precisely the entropy of $X$:</p> \[I(X; X) = H(X)\] </li> <li> <p>The mutual information can also be thought of as <em>the reduction in the uncertainty of one variable, due to the knowledge of the other</em>, i.e.,</p> \[I(X; Y) = H(Y) - H(Y|X) = H(X) - H(X|Y).\] </li> <li> <p>We can also write the mutual information as the sum of information in each variable, minus the total information contained (jointly) in $X$ and $Y$:</p> \[I(X; Y) = H(X) + H(Y) - H(X, Y)\] <p><img src="/blog/resources/mutualinfodiagram.png#center" alt=""/></p> </li> </ul> <h2 id="conditional-divergence-and-mutual-information">Conditional Divergence and Mutual Information</h2> <p>We can define conditional KL-divergence and conditional mutual information the same way we did for entropy.</p> <p>The <strong>conditional mutual information</strong> between $X$ and $Y$ given $Z$ is the reduction in the uncertainty of $X$ due to the knowledge of $Y$, given that $Z$ is known:</p> \[I(X;Y|Z) = H(X|Z) - H(X|Y, Z) \\\ = \mathbb{E}_{X, Y, Z \sim p(x, y, z)} \left[\log\frac{p(X, Y|Z)}{p(X|Z)p(Y|Z)}\right]\] <table> <tbody> <tr> <td>The <strong>conditional KL-divergence</strong> between the conditional distributions $p(y</td> <td>x)$ and $q(y</td> <td>x)$ is defined to be</td> </tr> </tbody> </table> \[D(p(y|x)||q(y|x)) = \mathbb{E}_{X, Y\sim p(x, y)}\left[\log\frac{p(Y|X)}{q(Y|X)}\right].\] <p>(Notice how the expectation is taken with regards to the joint distribution $p(x, y)$)</p> <p>We also have chain rule for mutual information:</p> \[I(X_1,\cdots,X_n; Y) = \sum_{i=1}^{n}I(X_i;Y|X_1,\cdots, X_{i-1}),\] <p>and for the KL-divergence: \(D(p(x_1, \cdots, x_n)||q(x_1, \cdots, x_n)) = \sum_{i=1}^{n} D(p(x_i|x_1,\cdots, x_{i-1})||q(x_i|x_1, \cdots,x_{i-1}))\) <strong><em>Remark:</em></strong> To prove any of the chain rules, just</p> <ol> <li>write the quantity in question (be it entropy, divergence, or mutual information) as the expected value of a function of the joint distributions,</li> <li>decompose the joint distributions inside the expectation by conditioning on variables one at a time,</li> <li>logarithm of products is equal to sum of logarithms,</li> <li>use linearity of expectation.</li> </ol> <h2 id="inequalities-and-bounds">Inequalities and Bounds</h2> <p>There are two main inequalities that are used in to provide bounds in the context of entropies: Jensen and log-sum inequalities. Before introducing them, some definitions:</p> <p>A function $f: (a, b) \to \R$ is <em>convex</em> if for every $x_1, x_2 \in (a, b)$ and $\lambda \in [0, 1]$,</p> \[f(\lambda x_1 +(1-\lambda) x_2) \leqslant \lambda f(x_1) + (1-\lambda)f(x_2).\] <p>If $f$ is twice differentiable, this is equivalent to $f''(x)\geqslant 0$ for all $x\in (a, b)$.</p> <p>Intuitively, $f$ is convex if it lies below the line segment connecting any two points on its graph.</p> <p>A function $f$ is <em>concave</em> if $-f$ is convex. This means that the inequality in the definition of convexity changes its direction, the second derivative is non-positive, and $f$ lies above the line segment connecting any two points on its graph.</p> <p><strong><em>Jensen’s Inequality:</em></strong> If $f$ is a convex function and $X$ is a random variable, $ \mathbb E [f(X)] \geqslant f(\mathbb E [X]).$ We particularly use this when $f$ is the logarithm function. If $f$ is concave, the direction of inequality changes.</p> <p><strong><em>Log-Sum Inequality:</em></strong> For non-negative numbers $a_1, a_2, \cdots, a_n$ and $b_1, b_2, \cdots, b_n$,</p> \[\sum_{i=1}^{n} {a_i\log\frac{a_i}{b_i}} \geqslant (\sum_{i=1}^n a_i) \log\frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}\] <p>with equality if and only if $\frac{a_i}{b_i} = \mathrm{constant}$.</p> <p>This inequality follows from the application of Jensen to the convex function $f(t) = t\log t$.</p> <p>Notice how the left hand side is similar to the definition of KL-divergence!</p> <p>Now, let’s see some consequences of the application of these inequalities:</p> <ul> <li> <table> <tbody> <tr> <td>KL-divergence is non-negative: $D(p</td> <td> </td> <td>q)\geqslant 0$ with equality only when $p=q$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Mutual information and conditional mutual information are both non-negative: $I(X; Y</td> <td>Z)\geqslant 0$ with equality only when $X$ and $Y$ are conditionally independent given $Z$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>(<em>Uniform distribution gives maximum entropy</em>) $H(X) \leqslant \log</td> <td>\mathcal{X}</td> <td>$ where $\mathcal{X}$ is the support of $X$ and equality if and only if $X$ is the uniform distribution over $\mathcal{X}$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>(<em>Conditioning reduces entropy</em>) Intuitively, having information about $Y$ can only decrease the uncertainty in $X$: $H(X</td> <td>Y) \leqslant H(X)$.</td> </tr> </tbody> </table> </li> <li> <p>(<em>Independence bound on entropy</em>) The joint entropy of $n$ variables can not be any larger than the sum of their individual entropies:</p> \[H(X_1, \cdots, X_n) \leqslant \sum_{i=1}^n H(X_i)\] <p>with equality if and only if $X_i$’s are independent.</p> </li> <li> <table> <tbody> <tr> <td>(<em>Convexity of KL-divergence</em>) $D(p</td> <td> </td> <td>q)$ is convex in the pair $(p, q)$; that is, for any two pairs of distributions $(p_1, q_1)$ and $(p_2, q_2)$ and any $\lambda \in [0, 1]$,</td> </tr> </tbody> </table> \[D(\lambda p_1 +(1-\lambda)p_2 || \lambda q_1 + (1-\lambda)q_2) \leqslant \lambda D(p_1||q_1) + (1-\lambda) D(p_2||q_2).\] </li> <li> <p>(<em>Concavity of entropy</em>) $H(p)$ is a concave function of $p$. Thus, the entropy of mixture of distributions is as large as the mixture of their entropies. This result follows easily from the convexity of KL-divergence, because:</p> </li> <li> <p>Let $u$ be the uniform distribution over $\mathcal{X}$. For any distribution $p$ over $\mathcal{X}$,</p> \[H(p) = H(u) - D(p||u) = \log {|\mathcal{X}|} - D(p||u)\] </li> </ul> <h2 id="data-processing-inequality">Data-Processing Inequality</h2> <table> <tbody> <tr> <td>Let $X\rightarrow Y \rightarrow Z$ be a Markov chain, that is, $Z$ is independent of $X$ given $Y$: $p(x, y, z) = p(x)p(y</td> <td>x)p(z</td> <td>y)$. Then,</td> </tr> </tbody> </table> \[I(X;Y) \geqslant I(X;Z).\] <ul> <li> <p>The data processing inequality shows that no clever manipulation of the data can improve the inference that can be made with that data.</p> </li> <li> <p>In particular, for any function $g$, $X \rightarrow Y \rightarrow g(Y)$ is a Markov chain, thus</p> \[I(X; Y) \geqslant I(X, g(Y))\] <p>which yields the previous claim about data manipulation.</p> </li> </ul> <h2 id="fanos-inequality">Fano’s Inequality</h2> <table> <tbody> <tr> <td>Suppose we know a random variable $Y$ and we want to guess the value of a correlated random variable $X$. Intuitively, knowing $Y$ should allow us to better guess the value of $X$. Fano’s inequality relates the probability of error in guessing $X$ to the conditional entropy $H(X</td> <td>Y)$.</td> </tr> </tbody> </table> <p>Suppose we use $\hat{X} = g(Y)$ to estimate $X$ after observing $Y$ ($g$ is a possibly random function). One can see that $X \rightarrow Y \rightarrow \hat X$ forms a Markov chain. We would like to bound the probability of error in estimating $X$ using $\hat X$, defined as</p> \[P_e = Pr[\hat X \neq X]\] <p><strong><em>(Fano’s inequality)</em></strong> For any estimator $\hat X$ such that $X\rightarrow Y \rightarrow \hat X$ forms a Markov chain, we have</p> \[H(P_e) + P_e \log {|\mathcal{X}|} \geqslant H(X|\hat X) \geqslant H(X|Y).\] <p>This can be weakened to</p> \[1 + P_e \log {|\mathcal X|} \geqslant H(X|Y)\] <p>or</p> \[P_e \geqslant \frac {H(X|Y) - 1}{\log {|\mathcal{X}|}}.\] <table> <tbody> <tr> <td>So, the larger $H(X</td> <td>Y)$ is, the more likely it is for our estimator to make an error.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Notice that $I(X; Y) = H(X) - H(X</td> <td>Y)$. So, if we fix the entropy of $X$, large mutual information between $X$ and $Y$ indicate that we can better estimate $X$ given $Y$.</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="blog"/><category term="information-theory"/><category term="entropy"/><category term="divergence"/><summary type="html"><![CDATA[A brief introduction to information theory, definitions and basic theorems.]]></summary></entry></feed>